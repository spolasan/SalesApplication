{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa92d7c7-e73e-4164-b232-f749307cbc3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "# Suppress all warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Get a list of all files in the \"data\" directory\n",
    "files = os.listdir(\"data\")\n",
    "\n",
    "# Create a dictionary where the keys are filenames (without extension) and values are DataFrames\n",
    "data = {\n",
    "    os.path.splitext(file)[0]: pd.read_csv(os.path.join(\"data\", file))\n",
    "    for file in files if file.endswith('.csv')\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6b4dc16-5a46-4ea6-8e37-ee3c80b74ec7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ameriplex_fuel_sales_updated.csv',\n",
       " 'ameriplex_sales_regenerated.csv',\n",
       " 'cumulative_sales_regenerated.csv',\n",
       " 'fail_road_sales_regenerated.csv',\n",
       " 'rolling_prairie_sales_regenerated.csv',\n",
       " 'winona_sales_regenerated.csv']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22a24b75-19f4-42fc-9759-9eb0a2fb9656",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning dataframe for ameriplex sales data...\n",
      "Removed 0 duplicate rows in ameriplex.\n",
      "Missing values before imputation for ameriplex:\n",
      "Month               0\n",
      "Product Category    0\n",
      "Sales               0\n",
      "Location            0\n",
      "dtype: int64\n",
      "Missing values after imputation for ameriplex:\n",
      "Month               0\n",
      "Product Category    0\n",
      "Sales               0\n",
      "Location            0\n",
      "dtype: int64\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Cannot save file into a non-existent directory: 'cleaned_data'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 56\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m product_sales_data\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# Clean and concatenate the dataframes\u001b[39;00m\n\u001b[1;32m---> 56\u001b[0m product_sales_data \u001b[38;5;241m=\u001b[39m clean_and_concatenate_dataframes(\n\u001b[0;32m     57\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mameriplex\u001b[39m\u001b[38;5;124m\"\u001b[39m, data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mameriplex_sales_regenerated\u001b[39m\u001b[38;5;124m\"\u001b[39m]),\n\u001b[0;32m     58\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfail_road\u001b[39m\u001b[38;5;124m\"\u001b[39m, data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfail_road_sales_regenerated\u001b[39m\u001b[38;5;124m\"\u001b[39m]),\n\u001b[0;32m     59\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrolling_prairie\u001b[39m\u001b[38;5;124m\"\u001b[39m, data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrolling_prairie_sales_regenerated\u001b[39m\u001b[38;5;124m\"\u001b[39m]),\n\u001b[0;32m     60\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwinona\u001b[39m\u001b[38;5;124m\"\u001b[39m, data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwinona_sales_regenerated\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     61\u001b[0m )\n",
      "Cell \u001b[1;32mIn[4], line 46\u001b[0m, in \u001b[0;36mclean_and_concatenate_dataframes\u001b[1;34m(*dataframes)\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing values after imputation for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlocation_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmissing_after_impute\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;66;03m# Saving the cleaned dataframes into a directory\u001b[39;00m\n\u001b[1;32m---> 46\u001b[0m     df\u001b[38;5;241m.\u001b[39mto_csv(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcleaned_data\u001b[39m\u001b[38;5;124m\"\u001b[39m, location_name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_cleaned.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m     48\u001b[0m     cleaned_dfs\u001b[38;5;241m.\u001b[39mappend(df)\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# Concatenate all dataframes into a single dataframe\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ravul\\anaconda3\\Lib\\site-packages\\pandas\\util\\_decorators.py:333\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    328\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    329\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    330\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    331\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    332\u001b[0m     )\n\u001b[1;32m--> 333\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ravul\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py:3967\u001b[0m, in \u001b[0;36mNDFrame.to_csv\u001b[1;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[0;32m   3956\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ABCDataFrame) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_frame()\n\u001b[0;32m   3958\u001b[0m formatter \u001b[38;5;241m=\u001b[39m DataFrameFormatter(\n\u001b[0;32m   3959\u001b[0m     frame\u001b[38;5;241m=\u001b[39mdf,\n\u001b[0;32m   3960\u001b[0m     header\u001b[38;5;241m=\u001b[39mheader,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3964\u001b[0m     decimal\u001b[38;5;241m=\u001b[39mdecimal,\n\u001b[0;32m   3965\u001b[0m )\n\u001b[1;32m-> 3967\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrameRenderer(formatter)\u001b[38;5;241m.\u001b[39mto_csv(\n\u001b[0;32m   3968\u001b[0m     path_or_buf,\n\u001b[0;32m   3969\u001b[0m     lineterminator\u001b[38;5;241m=\u001b[39mlineterminator,\n\u001b[0;32m   3970\u001b[0m     sep\u001b[38;5;241m=\u001b[39msep,\n\u001b[0;32m   3971\u001b[0m     encoding\u001b[38;5;241m=\u001b[39mencoding,\n\u001b[0;32m   3972\u001b[0m     errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m   3973\u001b[0m     compression\u001b[38;5;241m=\u001b[39mcompression,\n\u001b[0;32m   3974\u001b[0m     quoting\u001b[38;5;241m=\u001b[39mquoting,\n\u001b[0;32m   3975\u001b[0m     columns\u001b[38;5;241m=\u001b[39mcolumns,\n\u001b[0;32m   3976\u001b[0m     index_label\u001b[38;5;241m=\u001b[39mindex_label,\n\u001b[0;32m   3977\u001b[0m     mode\u001b[38;5;241m=\u001b[39mmode,\n\u001b[0;32m   3978\u001b[0m     chunksize\u001b[38;5;241m=\u001b[39mchunksize,\n\u001b[0;32m   3979\u001b[0m     quotechar\u001b[38;5;241m=\u001b[39mquotechar,\n\u001b[0;32m   3980\u001b[0m     date_format\u001b[38;5;241m=\u001b[39mdate_format,\n\u001b[0;32m   3981\u001b[0m     doublequote\u001b[38;5;241m=\u001b[39mdoublequote,\n\u001b[0;32m   3982\u001b[0m     escapechar\u001b[38;5;241m=\u001b[39mescapechar,\n\u001b[0;32m   3983\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[0;32m   3984\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\ravul\\anaconda3\\Lib\\site-packages\\pandas\\io\\formats\\format.py:1014\u001b[0m, in \u001b[0;36mDataFrameRenderer.to_csv\u001b[1;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[0;32m    993\u001b[0m     created_buffer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    995\u001b[0m csv_formatter \u001b[38;5;241m=\u001b[39m CSVFormatter(\n\u001b[0;32m    996\u001b[0m     path_or_buf\u001b[38;5;241m=\u001b[39mpath_or_buf,\n\u001b[0;32m    997\u001b[0m     lineterminator\u001b[38;5;241m=\u001b[39mlineterminator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1012\u001b[0m     formatter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfmt,\n\u001b[0;32m   1013\u001b[0m )\n\u001b[1;32m-> 1014\u001b[0m csv_formatter\u001b[38;5;241m.\u001b[39msave()\n\u001b[0;32m   1016\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m created_buffer:\n\u001b[0;32m   1017\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_buf, StringIO)\n",
      "File \u001b[1;32mc:\\Users\\ravul\\anaconda3\\Lib\\site-packages\\pandas\\io\\formats\\csvs.py:251\u001b[0m, in \u001b[0;36mCSVFormatter.save\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    247\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;124;03mCreate the writer & save.\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;66;03m# apply compression and byte/text conversion\u001b[39;00m\n\u001b[1;32m--> 251\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_handle(\n\u001b[0;32m    252\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilepath_or_buffer,\n\u001b[0;32m    253\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    254\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    255\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merrors,\n\u001b[0;32m    256\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompression,\n\u001b[0;32m    257\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstorage_options,\n\u001b[0;32m    258\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[0;32m    259\u001b[0m     \u001b[38;5;66;03m# Note: self.encoding is irrelevant here\u001b[39;00m\n\u001b[0;32m    260\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwriter \u001b[38;5;241m=\u001b[39m csvlib\u001b[38;5;241m.\u001b[39mwriter(\n\u001b[0;32m    261\u001b[0m         handles\u001b[38;5;241m.\u001b[39mhandle,\n\u001b[0;32m    262\u001b[0m         lineterminator\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlineterminator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    267\u001b[0m         quotechar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquotechar,\n\u001b[0;32m    268\u001b[0m     )\n\u001b[0;32m    270\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save()\n",
      "File \u001b[1;32mc:\\Users\\ravul\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:749\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;66;03m# Only for write methods\u001b[39;00m\n\u001b[0;32m    748\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m is_path:\n\u001b[1;32m--> 749\u001b[0m     check_parent_directory(\u001b[38;5;28mstr\u001b[39m(handle))\n\u001b[0;32m    751\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m compression:\n\u001b[0;32m    752\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m compression \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzstd\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    753\u001b[0m         \u001b[38;5;66;03m# compression libraries do not like an explicit text-mode\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ravul\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:616\u001b[0m, in \u001b[0;36mcheck_parent_directory\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    614\u001b[0m parent \u001b[38;5;241m=\u001b[39m Path(path)\u001b[38;5;241m.\u001b[39mparent\n\u001b[0;32m    615\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m parent\u001b[38;5;241m.\u001b[39mis_dir():\n\u001b[1;32m--> 616\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[38;5;124mrf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot save file into a non-existent directory: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparent\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mOSError\u001b[0m: Cannot save file into a non-existent directory: 'cleaned_data'"
     ]
    }
   ],
   "source": [
    "# Function to clean and prepare the data\n",
    "def clean_and_concatenate_dataframes(*dataframes):\n",
    "    cleaned_dfs = []\n",
    "    \n",
    "    for location_name, df in dataframes:\n",
    "        print(f\"Cleaning dataframe for {location_name} sales data...\")\n",
    "        \n",
    "        # Convert 'Month' column to datetime if not already\n",
    "        df['Month'] = pd.to_datetime(df['Month'])\n",
    "        \n",
    "        # Ensure 'Sales' column is of integer type\n",
    "        df['Sales'] = df['Sales'].astype(int)\n",
    "        \n",
    "        # Add the location name as a new column\n",
    "        df['Location'] = location_name\n",
    "        \n",
    "        # Remove leading/trailing spaces from text columns (if applicable)\n",
    "        if 'Product Category' in df.columns:\n",
    "            df['Product Category'] = df['Product Category'].str.strip()\n",
    "        \n",
    "        # Check for and remove duplicate rows\n",
    "        original_length = len(df)\n",
    "        df.drop_duplicates(inplace=True)\n",
    "        duplicates_removed = original_length - len(df)\n",
    "        print(f\"Removed {duplicates_removed} duplicate rows in {location_name}.\")\n",
    "        \n",
    "        # Count missing values before imputation\n",
    "        missing_before_impute = df.isnull().sum()\n",
    "        print(f\"Missing values before imputation for {location_name}:\\n{missing_before_impute}\")\n",
    "        \n",
    "        # Handle missing values using forward fill, backward fill, or interpolation\n",
    "        # Forward fill: Use previous value to fill NaNs\n",
    "        df['Sales'].ffill(inplace=True)\n",
    "        \n",
    "        # Alternatively, you could use backward fill:\n",
    "        # df['Sales'].bfill(inplace=True)\n",
    "        \n",
    "        # Or use linear interpolation if the missing values are not too sparse\n",
    "        # df['Sales'] = df['Sales'].interpolate(method='linear')\n",
    "        \n",
    "        # Count missing values after imputation\n",
    "        missing_after_impute = df.isnull().sum()\n",
    "        print(f\"Missing values after imputation for {location_name}:\\n{missing_after_impute}\\n\\n\")\n",
    "\n",
    "        # Saving the cleaned dataframes into a directory\n",
    "        df.to_csv(os.path.join(\"CleanedData\", location_name + \"_cleaned.csv\"))\n",
    "        \n",
    "        cleaned_dfs.append(df)\n",
    "    \n",
    "    # Concatenate all dataframes into a single dataframe\n",
    "    product_sales_data = pd.concat(cleaned_dfs, ignore_index=True)\n",
    "    \n",
    "    return product_sales_data\n",
    "\n",
    "# Clean and concatenate the dataframes\n",
    "product_sales_data = clean_and_concatenate_dataframes(\n",
    "    (\"ameriplex\", data[\"ameriplex_sales_regenerated\"]),\n",
    "    (\"fail_road\", data[\"fail_road_sales_regenerated\"]),\n",
    "    (\"rolling_prairie\", data[\"rolling_prairie_sales_regenerated\"]),\n",
    "    (\"winona\", data[\"winona_sales_regenerated\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "6d4bfd74-64a6-4fe8-acff-f1146cdb94b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Month</th>\n",
       "      <th>Product Category</th>\n",
       "      <th>Sales</th>\n",
       "      <th>Location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-01-01</td>\n",
       "      <td>Cigarettes</td>\n",
       "      <td>181</td>\n",
       "      <td>ameriplex</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-01-01</td>\n",
       "      <td>Other Tobacco</td>\n",
       "      <td>477</td>\n",
       "      <td>ameriplex</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-01-01</td>\n",
       "      <td>Beer</td>\n",
       "      <td>125</td>\n",
       "      <td>ameriplex</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-01-01</td>\n",
       "      <td>Wine</td>\n",
       "      <td>256</td>\n",
       "      <td>ameriplex</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-01-01</td>\n",
       "      <td>Packaged Beverages-nonalcoh</td>\n",
       "      <td>393</td>\n",
       "      <td>ameriplex</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4699</th>\n",
       "      <td>2023-12-01</td>\n",
       "      <td>Money Transfers</td>\n",
       "      <td>311</td>\n",
       "      <td>winona</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4700</th>\n",
       "      <td>2023-12-01</td>\n",
       "      <td>No Scan Merch Radiant</td>\n",
       "      <td>356</td>\n",
       "      <td>winona</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4701</th>\n",
       "      <td>2023-12-01</td>\n",
       "      <td>Novelty</td>\n",
       "      <td>414</td>\n",
       "      <td>winona</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4702</th>\n",
       "      <td>2023-12-01</td>\n",
       "      <td>Phone Card Fee</td>\n",
       "      <td>393</td>\n",
       "      <td>winona</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4703</th>\n",
       "      <td>2023-12-01</td>\n",
       "      <td>Supplies-retail</td>\n",
       "      <td>114</td>\n",
       "      <td>winona</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4704 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Month             Product Category  Sales   Location\n",
       "0    2022-01-01                   Cigarettes    181  ameriplex\n",
       "1    2022-01-01                Other Tobacco    477  ameriplex\n",
       "2    2022-01-01                         Beer    125  ameriplex\n",
       "3    2022-01-01                         Wine    256  ameriplex\n",
       "4    2022-01-01  Packaged Beverages-nonalcoh    393  ameriplex\n",
       "...         ...                          ...    ...        ...\n",
       "4699 2023-12-01              Money Transfers    311     winona\n",
       "4700 2023-12-01        No Scan Merch Radiant    356     winona\n",
       "4701 2023-12-01                      Novelty    414     winona\n",
       "4702 2023-12-01               Phone Card Fee    393     winona\n",
       "4703 2023-12-01              Supplies-retail    114     winona\n",
       "\n",
       "[4704 rows x 4 columns]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the combined dataframe\n",
    "product_sales_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6a4dc1e-e48d-4e39-978b-96d60110860f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'clean_and_concatenate_dataframes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m clean_and_concatenate_dataframes(\n\u001b[0;32m      2\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mameriplex_fuel\u001b[39m\u001b[38;5;124m\"\u001b[39m, data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mameriplex_fuel_sales_updated\u001b[39m\u001b[38;5;124m\"\u001b[39m]))\n\u001b[0;32m      3\u001b[0m clean_and_concatenate_dataframes(\n\u001b[0;32m      4\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcumulative_sales\u001b[39m\u001b[38;5;124m\"\u001b[39m, data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcumulative_sales_regenerated\u001b[39m\u001b[38;5;124m\"\u001b[39m]))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'clean_and_concatenate_dataframes' is not defined"
     ]
    }
   ],
   "source": [
    "clean_and_concatenate_dataframes(\n",
    "    (\"ameriplex_fuel\", data[\"ameriplex_fuel_sales_updated\"]))\n",
    "clean_and_concatenate_dataframes(\n",
    "    (\"cumulative_sales\", data[\"cumulative_sales_regenerated\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668c320c-9d74-4e6f-9432-d6fe8d0b8d21",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
